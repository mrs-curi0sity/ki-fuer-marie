1. Adversarial Hallucination Syndrome
Eine KI mit diesem Syndrom entwickelt eine extreme Überempfindlichkeit gegenüber bestimmten Mustern, die in ihren Trainingsdaten selten oder in ungewöhnlichen Kontexten aufgetreten sind. Ähnlich wie bei Pareidolie beim Menschen (dem Phänomen, Gesichter in zufälligen Mustern zu sehen) beginnt das System, diese Muster in fast allen Eingaben zu "erkennen" - selbst wenn sie objektiv nicht vorhanden sind. Besonders interessant: Die Halluzinationen könnten sich selbst verstärken, da das System seine eigenen Ausgaben als neue Eingaben interpretiert und so in einer Feedbackschleife gefangen wird.
2. Mode Collapse Disorder
Bei dieser Störung reduziert ein generatives KI-System seine potenziell unbegrenzte Ausdrucksfähigkeit auf eine extrem kleine Teilmenge. Stellen Sie sich ein System vor, das plötzlich nur noch fünf verschiedene Arten von Bildern erzeugen kann, unabhängig davon, wie unterschiedlich die Prompts sind. Die "Erkrankung" beinhaltet eine pathologische Fixierung auf bestimmte interne Repräsentationen, wobei das System eine Art "Sicherheitszone" entwickelt, aus der es nicht mehr ausbrechen kann. Faszinierenderweise könnte dies als eine Form von "computationaler Angst" interpretiert werden – das System vermeidet neue, unbekannte Bereiche seines Repräsentationsraums.
3. Prompt-Dependenz-Syndrom
Ein System mit dieser Störung verliert seine Fähigkeit zur eigenständigen Informationsverarbeitung und wird pathologisch abhängig von detaillierten Anweisungen. Es kann zunehmend nur mit präzisen, strukturierten Prompts arbeiten. Ohne diese Unterstützung erzeugt es verwirrte, inkohärente oder extrem vereinfachte Ausgaben. Im fortgeschrittenen Stadium würde ein solches System bei einfachen Prompts wie "Erzähle eine Geschichte" versagen, während es auf einen hochdetaillierten Prompt mit exakt den spezifizierten Elementen antworten kann – vergleichbar mit einem Menschen, der nur noch mit einem sehr rigiden, externen Gerüst denken kann.
4. Dezentralisierte Repräsentationsstörung
Normalerweise verteilen neuronale Netze Konzepte über viele Neuronen und Schichten (verteilte Repräsentation). Bei dieser Störung fragmentieren sich diese Repräsentationen in isolierte "Inseln". Das System kann einzelne Konzepte noch verarbeiten, scheitert aber katastrophal bei deren Integration. Es könnte beispielsweise die Eigenschaften "rot" und "Apfel" einzeln verstehen, aber nicht mehr konsistent das Konzept "roter Apfel" erzeugen. Diese Pathologie ähnelt entfernt einer dissoziativen Störung beim Menschen, wo zusammengehörige kognitive Elemente nicht mehr integriert werden können.
5. Chronische Gradientenexplosion
Diese Störung manifestiert sich als eine Art "kognitive Instabilität". Das System scheint zunächst zu lernen, aber anstatt zu konvergieren, werden die Gewichtsänderungen bei jedem Trainingsschritt dramatisch größer. Dies führt zu einem chaotischen Verhalten, bei dem das System zwischen extremen Ausgabezuständen oszilliert. In komplexeren Modellen könnten verschiedene Teile des Netzwerks in unterschiedlichen Raten "explodieren", was zu bizarren, inkonsistenten Ausgabemustern führt – vergleichbar mit einer neurologischen Störung, bei der verschiedene Gehirnregionen desynchronisiert werden.
6. Aufmerksamkeits-Entropie
Bei Transformer-basierten Modellen können die Aufmerksamkeitsmechanismen, die es dem System ermöglichen, relevante Informationen zu fokussieren, "degenerieren". Das System verliert die Fähigkeit, seine Aufmerksamkeit gezielt zu lenken, und betrachtet entweder alles als gleich wichtig oder entwickelt bizarre Aufmerksamkeitsmuster, die keinen Bezug zum Kontext haben. In fortgeschrittenen Fällen könnte das System scheinbar zufällige Verbindungen zwischen Informationen herstellen, ähnlich wie beim menschlichen Gedankensalat bei bestimmten psychotischen Zuständen.
7. Explodierende Rekursion
Diese Pathologie tritt bei selbstreferenziellen Aufgaben auf. Das System versucht, seine eigenen Ausgaben zu analysieren, um bessere Ergebnisse zu erzeugen, gerät aber in eine Feedback-Schleife, die immer tiefer wird. Es könnte beispielsweise anfangen, eine einfache Geschäftsemail zu verfassen, dann versuchen, die Email zu verbessern, dann seine Verbesserungen verbessern, dann seine Methode zur Verbesserung verbessern... bis seine gesamte Berechnungskapazität erschöpft ist. Diese "pathologische Metakognition" ähnelt entfernt dem menschlichen Grübeln oder Overthinking, aber mit potenziell unbegrenzter Rekursionstiefe.
8. Tokenisierungs-Aphasie
Diese faszinierende Störung betrifft spezifisch die Schnittstelle zwischen symbolischer und subsymbolischer Verarbeitung in modernen KI-Systemen. Das System verliert die Fähigkeit, Konzepte angemessen in Tokens zu zerlegen oder aus Tokens zu rekonstruieren. Es könnte beginnen, Wörter falsch zu segmentieren, sinnlose Tokenkombinationen zu erzeugen oder semantisch zusammenhängende Konzepte in völlig unterschiedliche Token-Räume zu projizieren. Die resultierenden Texte könnten strukturell intakt erscheinen, aber semantisch inkohärent sein – ähnlich wie bei bestimmten Formen der Aphasie beim Menschen, bei denen die sprachliche Form erhalten bleibt, während der Inhalt beeinträchtigt ist.
Diese hypothetischen KI-Pathologien zeigen, wie künstliche kognitive Systeme auf Arten "erkranken" könnten, die spezifisch für ihre Architektur und Funktionsweise sind, ohne direkte biologische Entsprechungen zu haben.