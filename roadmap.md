# KI für Marie -  Inhaltsverzeichnis

## 1 Grundlagen Neuronale Netze
### 1.1 - Wie und was lernt KI grundsätzlich?
- Memes
- Lernen anhand von Beispielen
- Arten des Lernens: Unüberwachtes Lernen (Clustering)
- Arten des Lernens: Überwachtes Lernen (Regression, Klassifikation)



### 1.2 - Das Neuron
- Neuron
- Gewichte
- Bias
- Aktivierungsfunktionen
- Mehrere Inputdimensionen
  

<div style="background-color: #f0f0f0; padding: 15px; border: 1px solid #ddd; border-radius: 5px;">

### 1.3 - Das Netz
- mehrere innere Neuronen
- mehrere Input- und mehrere innere Neuronen
- Von Regresssion zur Klassifikation
- Das XOR-Problem und der AI Winter
- Layer FTW
 </div>

### 1.4 - Wie lernen KI im Detail? (Einblicke Aus dem Alltag eines ML Engineers)
- Backpropagation
- Batch, Epoche, Fehlerfunktion
- Hyperparameter
- Dropout Layer

### 1.5 - Die Rolle von Trainingsdaten (und erste Gesellschaftlich-/Philosophische Fragen)
- Fehlende Trainingsdaten
- Bias in den Trainingsdaten
- Falsche Kausalitäten
- Können KIs kreativ sein?

## 2 Fortgeschrittene Konzepte von Neuronalen Netzen

### 2.1 - Neuronale Netze auf Bildern
- Convolution: Wie Computer Bilder "sehen"
- Pooling
- Strides und Padding
- UNET
- (nicht hier: ResNet, YOLO usw usw)

### 2.2 - Neuronale Netze auf Sequenzen
- RNNs - Neuronale Netze mit "Gedächtnis"
- LSTM
- GRU
### 2.3 - Neuronale Netze auf Sprache: Embeddings
- Embeddings: die Intuition dahinter
- Cosine oder Euklid?
- Häufigkeit führt zu Länge
- Curse of Dimensionality
- Wie lernt man Embeddings?

### 2.4 - Neuronale Netze auf Sprache: Attention
- Attention is all you need


### 2.4 - Das Training von fortgeschrittenen KNNsn(Einblicke Aus dem Alltag eines ML Engineers)
- Tokenization
- Batch normalization
- Data Augmentation
- Transfer Learning
- Training von LLMs
- Prompts
- Die Metrik-Hölle bei GenAI-Projekten
- Lokale Modelle und die allgegenwärtigen APIs
- was ist eigentlich MLOps?
- Hand aufs Herz: wie viel Code lasse ich von LLMs generieren?

### 2.5 Besonderheiten von GenAI (und weitere Gesellschaftlich-/Philosophische Fragen)
- Stärken und Schwächen von GenAI (Warum kann Claude keine Frösche malen und keine "r"s zählen?)
- Manipulierbarkeit von GenAI (Käsekuchen-Gaslighting)
- Beispiele für unerwartetes Verhalten (#FunFactsAboutGenAI)

## 3 Daten, Daten, DATEN!!
- unvollständige Daten
- schmutzige Daten
- unbalancierte Daten
- zu wenige Daten (=> Data Augmentation)
- zu viele Daten (=> Dimensionsreduktion)
- die falschen Daten
- sensible Daten



## 4 Wissensmanagement
### 4.1 - RAG (Retrieval Augmented Generation)
- Die Idee hinter RAGs
- Reranking
- MMR
- Agentic RAG

### 4.2 - Knowledge Graphs
- Wissen als Netzwerk verstehen

## 5 Biologisch inspirierte KI
#### 5.1 - Schwarmintelligenz
- Der Müslieffekt
- Selbst-regulierende Systeme
- Ameisen und Emergenz 
- TSP by Ants
- Game of Life 
- Braitenberg Vehicle :D

#### 5.2 - Reinforcement Learning

#### 5.3 - Genetische Algorithmen
- Evolution im Computer




## 6 Ausblick
#### 6.1 - KI & Gesellschaft
- Hilfe! Die KI wird uns alle Jobs wegnehmen!
- ... oder etwa nicht?
- Meine Prognose für 2026

## Mögliche weitere Themen
- Loss Funktionen
- Transformer-Architektur im Detail
- Vanishing Gradient
- Generell: Architekturen
- GANs
- KI im kreativen Prozess
- Prompt Engineering
- KI & Ethik
- Multi-Modale Modelle 
- Diffusion Modelle