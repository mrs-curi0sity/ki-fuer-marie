# KI für Marie -  Inhaltsverzeichnis

## 1 Grundlagen Neuronale Netze
#### 1.1 - Wie funktioniert ein Neuron?
- Neuron
- Gewichte
- Bias
- Aktivierungsfunktionen
- Mehrere Inputs

#### 1.2 - wie lernt ein Neuron?
- Backpropagation
- Hyperparameter
  
### 1.3 Layer
- Von Regresssion zu Klassifikation
- Frust über das XOR-Problem
- Layer FTW

#### 1.4 - Neuronale Netze auf Bildern
- Convolution: Wie Computer Bilder "sehen"
- Kantendetektoren
- Pooling

#### 1.5 - RNNs
- Neuronale Netze mit "Gedächtnis"

## 2 LLMs
#### 020 - wie kann ein Neuronales Netz Sprache verarbeiten?
- Tokenization
- Embeddings
- Anschauliche Beispiele
- Wie lernt man Embeddings?

#### 021 - Large Language Models (LLMs)
- Attention is all you need
- Training und seine Auswirkungen
- Die Metrik-Hölle

## 3 - Das Verhalten von GenAI
- Hand aufs Herz: wie viel Code lasse ich von LLMs generieren?
- Stärken und Schwächen (aka Warum scheitert GenAI an Hässlichkeit?)
- Beispiele für unerwartetes Verhalten (#FunFactsAboutGenAI)

## 4 Daten, Daten, DATEN!!
- unvollständige Daten
- schmutzige Daten
- unbalancierte Daten
- zu wenige Daten
- zu viele Daten (Dimensionen)
- die falschen Daten
- sensible Daten


## 5 Wissensmanagement
#### 050 - RAG (Retrieval Augmented Generation)
- Die Idee hinter RAGs
- Reranking
- MMR
- Agentic RAG

#### 051 - Knowledge Graphs
- Wissen als Netzwerk verstehen

## 6 Biologisch inspirierte KI
#### 060 - Schwarmintelligenz
- Der Müslieffekt
- Selbst-regulierende Systeme
- Ameisen und Emergenz 
- TSP by Ants
- Game of Life 
- Braitenberg Vehicle :D

#### 061 - Reinforcement Learning

#### 062 - Genetische Algorithmen
- Evolution im Computer




## 7 Ausblick
#### 060 - KI & Gesellschaft
- Hilfe! Die KI wird uns alle Jobs wegnehmen!
- ... oder etwa nicht?
- Meine Prognose für 2026

## Mögliche weitere Themen
- Loss Funktionen
- Transformer-Architektur im Detail
- Vanishing Gradient
- Generell: Architekturen
- GANs
- KI im kreativen Prozess
- Prompt Engineering
- KI & Ethik
- Multi-Modale Modelle 
- Diffusion Modelle